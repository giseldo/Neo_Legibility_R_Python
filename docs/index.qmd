---
title: "Neo Legibility Effort Model"
subtitle: "Giseldo da Silva Neo"
format:
  revealjs: 
    theme: default
    slide-number: true
editor: visual

---

## Hipótese {.smaller}

A polaridade do sentimento (ou tom) da descrição da User Story, a subjetividade da descrição da User Story e a legibilidade da descrição da da User Story são bons preditores da estimativa de esforço.

## Conjunto de dados {.smaller}

```{python}
import pandas as pd
import matplotlib.pyplot as plt
from textblob import TextBlob
import textstat
from sklearn.svm import SVR
from sklearn.feature_extraction.text import TfidfVectorizer
```

::: columns
::: {.column width="35%"}
#### Conjunto de dados

O [Neo Dataset](https://github.com/giseldo/neodataset) contém as User Story com seus respectivos Story Points de 34 Projetos de desenvolvimento de software.
:::

::: {.column width="3%"}
:::

::: {.column width="62%"}
```{python}
name = '7764'
filename = '../data/{}.csv'.format(name)
df = pd.read_csv(filename)
df[["issuekey","title", "description", "storypoints"]].head(2)
```
:::
:::

::: footer
Made by Giseldo
:::

## Variáveis {.smaller}

As variáveis do conjunto de dados são:

| Variável         |Tipo                | Preditora ou Rótulo | Descrição       |
|------------------|--------------------|---------------------|-----------------|
| issuekey         | categórica simples |     -     | Chave                     |
| created          | data e hora        |     -     | Data de criação           |
| title            | texto              | preditora | Título da User Story      |
| description      | texo               | preditora | Descrição da User Story   |
| storypoints      | numérico discreto  | rótulo    | Story Point da User Story |


::: footer
Made by Giseldo
:::

## Transformação {.smaller}

### Projeto 7764

A coluna **title** e **description** foi transformada em uma única coluna chamada **context**. As outras colunas foram removidas pois não trazem dados preditivos.

```{python}
df["context"] = df["title"] + df["description"]
df = df.drop(['created', 'issuekey', 'title', 'description'], axis=1)
df['context'] = df['context'].astype(str)
df.head()
```

::: footer
Made by Giseldo
:::

## Story Point {.smaller}

### Projeto 7764

Detecado presença de outliers.

```{python}
value_count = df['storypoints'].value_counts().sort_index()
```

::: columns
::: {.column width="60%"}
```{python}
plt.figure(figsize=(6, 4))
value_count.plot(kind='bar', color=['blue'])
plt.xlabel('Story Points')
plt.ylabel('Contagem')
plt.show()
```
:::

::: {.column width="40%"}
```{python}
plt.figure(figsize=(4,4))
plt.boxplot(df['storypoints'])
plt.xticks([1], ['Story Points'])
plt.show()
```
:::
:::

::: footer
Made by Giseldo
:::

## Outliers {.smaller auto-animate="true"}

### Projeto 7764

Foram removidos os Story Poits com 2 desvios padrão antes e depois da média.

``` r
mean = df['storypoints'].mean()
std_dev = df['storypoints'].std()
outlier_cutoff = 2 * std_dev
df_clean = df[(df['storypoints'] >= mean - outlier_cutoff) & (df['storypoints'] < mean + outlier_cutoff)]
```

```{python}
mean = df['storypoints'].mean()
std_dev = df['storypoints'].std()
outlier_cutoff = 2 * std_dev
df_clean = df[(df['storypoints'] >= mean - outlier_cutoff) & (df['storypoints'] < mean + outlier_cutoff)]
```

Antes da remoção 355 observações. Depois 352 observações.

::: footer
Made by Giseldo
:::

## Story Point (sem outliers) {.smaller .scrollable}

### Projeto 7764

```{python}
value_count = df_clean['storypoints'].value_counts().sort_index()
plt.show()
```

::: columns
::: {.column width="60%"}
```{python}
plt.figure(figsize=(6, 4))
value_count.plot(kind='bar', color=['blue'])
plt.xlabel('Story Points')
plt.ylabel('Contagem')
plt.show()
```
:::

::: {.column width="40%"}
```{python}
plt.figure(figsize=(4, 4))
plt.boxplot(df_clean['storypoints'])
plt.xticks([1], ['Story Points'])
plt.show()
```
:::
:::

::: footer
Made by Giseldo
:::



## Data frame {.smaller }

### Projeto 7764

```{python}
def mean_absolute_error(y_true, y_pred):
  """
  Calcula o Mean Absolute Error (MAE), entre os valores verdadeiros (y_true) e os valores previstos (y_pred)

  Args:
    y_true: Uma lista ou array Numpy dos valores verdadeiros.
    y_pred: Uma lista ou array NumPy dos valores previstos.

  Returns:
    mae: O Mean Absolute Error entre y_true e y_pred
  """
  if len(y_true) != len(y_pred):
    raise ValueError('Os tamanhos de y_true e y_pred devem ser iguais')
  absolute_Errors =[abs(true-pred) for true, pred in zip(y_true, y_pred)]
  mae = sum(absolute_Errors) / len(y_true)
  return mae
```

```{python}
num_linhas_treino = int(len(df_clean) * 0.7)
dados_treino = df_clean.iloc[:num_linhas_treino]
dados_teste = df_clean.iloc[num_linhas_treino:]
media_sp = dados_treino['storypoints'].mean()

lista_y_pred = [media_sp] * len(dados_teste)
mae_media_sp = mean_absolute_error(dados_teste['storypoints'], lista_y_pred)

df_results = pd.DataFrame(data=[['Media', mae_media_sp, 'blue']], columns=['modelo', 'MAE Teste', "color"])

colunas = ['gunning_fog', 'polarity','subjectivity']

dados_treino['gunning_fog'] = dados_treino['context'].apply(textstat.gunning_fog)
dados_treino['polarity'] = dados_treino['context'].apply(lambda x: TextBlob(x).sentiment.polarity)
dados_treino['subjectivity'] = dados_treino['context'].apply(lambda x: TextBlob(x).sentiment.subjectivity)

dados_teste['gunning_fog'] = dados_teste['context'].apply(textstat.gunning_fog)
dados_teste['polarity'] = dados_teste['context'].apply(lambda x: TextBlob(x).sentiment.polarity)
dados_teste['subjectivity'] = dados_teste['context'].apply(lambda x: TextBlob(x).sentiment.subjectivity)
```

```{python}
dados_teste.head()
```

::: footer
Made by Giseldo
:::

## TF-IDF Model {.smaller }

### Projeto 7764

No TF-IDF a coluna context é transformada em uma matriz. Cada palavra é uma coluna dessa matriz. Por isso é interessante o uso de técnicas de pré-processamento de texto (para diminuir o número de palavras que não são relevantes).

| Variável         |Tipo                | Descrição                     |
|------------------|--------------------|-------------------------------|
| context          | texto              | título + descrição            |
| storypoint       | numérico contínuo  | Story Point da User Story     |

::: footer
Made by Giseldo
:::

## Neo Legibility {.smaller }

### Projeto 7764

Atributos extraídos da User Story (*)

| Variável         |Tipo                | Descrição                     |
|------------------|--------------------|-------------------------------|
| *gunning_fog      | numérico contínuo  | Legibilidade da User Story    |
| *polarity         | numérico contínuo  | Sentimento da User Story      |
| *subjectivity     | numérico contínuo  | Subjetividade da User Story   |
| storypoint       | numérico contínuo  | Story Point da User Story   |

::: footer
Made by Giseldo
:::

## Neo Legibility - Correlação {.smaller }

### Projeto 7764

Existe uma correlação fraca entre a legibilidade e os storypoints.

```{python}
dados_treino[['gunning_fog', 'polarity', 'subjectivity', 'storypoints']].corr()
```
::: footer
Made by Giseldo
:::

## Resultado comparação entre os MAE dos modelos {.smaller}

### Projeto 7764

O Modelo que tem o menor MAE (erro médio absoluto) é o Neo Legibility.

```{python}
model = SVR()
model.fit(dados_treino[colunas], dados_treino['storypoints'])
y_pred = model.predict(dados_teste[colunas])
mae_leg = mean_absolute_error(dados_teste['storypoints'], y_pred)
df_results = df_results.append({'modelo':'Neo Legibility (SVM)', 'MAE Teste': mae_leg, 'color': 'orange'}, ignore_index=True)

vec = TfidfVectorizer(max_features=50)
tfidf_matrix_treino = vec.fit_transform(dados_treino['context'])
tfidf_matrix_teste = vec.transform(dados_teste['context'])

model = SVR()
model.fit(tfidf_matrix_treino, dados_treino['storypoints'])
y_pred = model.predict(tfidf_matrix_teste)
mae_tfidf = mean_absolute_error(dados_teste['storypoints'], y_pred)
df_results = df_results.append({'modelo':'TF-IDF (SVM)', 'MAE Teste': mae_tfidf, 'color': 'green'}, ignore_index=True)

plt.figure()
df_results = df_results.sort_values(by='MAE Teste')
plt.bar(df_results['modelo'], df_results['MAE Teste'], color=df_results['color'])
plt.ylim(1.1, 1.4)
plt.title('Comparação do MAE entre os modelos')
plt.xlabel('Modelos')
plt.ylabel('MAE')
plt.show()
```

::: footer
Made by Giseldo
:::
