mae_tfidf = mean_absolute_error(dados_teste["storypoints"], y_pred)
df_results = df_results.append({'modelo':'mae_tfidf', 'MAE Teste': mae_tfidf}, ignore_index=True)
df_results
plt.figure(figsize=(10, 8))
df_results = df_results.sort_values(by="MAE Teste")
df_results.plot.barh(y='MAE Teste', x ='modelo')
plt.barv(df_results['MAE Teste'], df_results['modelo'])
plt.title("Comparação do MAE entre os modelos")
plt.xlabel("MAE")
plt.ylabel("Modelos")
plt.show()
plt.figure()
df_results = df_results.sort_values(by="MAE Teste")
df_results.plot.barh(y='MAE Teste', x ='modelo')
plt.barv(df_results['MAE Teste'], df_results['modelo'])
plt.title("Comparação do MAE entre os modelos")
plt.xlabel("MAE")
plt.ylabel("Modelos")
plt.show()
plt.figure()
df_results = df_results.sort_values(by="MAE Teste")
df_results.plot.barh(y='MAE Teste', x ='modelo')
plt.bar(df_results['MAE Teste'], df_results['modelo'])
plt.title("Comparação do MAE entre os modelos")
plt.xlabel("MAE")
plt.ylabel("Modelos")
plt.show()
plt.figure()
df_results = df_results.sort_values(by="MAE Teste")
df_results.plot.barh(y='MAE Teste', x ='modelo')
plt.bar(df_results['modelo'], df_results['MAE Teste'])
plt.title("Comparação do MAE entre os modelos")
plt.xlabel("MAE")
plt.ylabel("Modelos")
plt.show()
plt.figure()
df_results = df_results.sort_values(by="MAE Teste")
#df_results.plot.barh(y='MAE Teste', x ='modelo')
plt.bar(df_results['modelo'], df_results['MAE Teste'])
plt.title("Comparação do MAE entre os modelos")
plt.xlabel("MAE")
plt.ylabel("Modelos")
plt.show()
plt.figure()
df_results = df_results.sort_values(by="MAE Teste")
plt.bar(df_results['modelo'], df_results['MAE Teste'])
plt.title("Comparação do MAE entre os modelos")
plt.xlabel("Modelos")
plt.ylabel("MAE")
plt.show()
vec = TfidfVectorizer(max_features=50)
tfidf_matrix_treino = vec.fit_transform(dados_treino["context"])
tfidf_matrix_teste = vec.transform(dados_teste["context"])
import logging;
logging.getLogger().setLevel(logging.WARNING)
plt.figure(figsize=(8, 6))
value_count.plot(kind='bar')
plt.title("Value Counts dos Story Points do Projeto 7764")
plt.xlabel("Valores do Story Point")
plt.ylabel("Contagem de vezes dos Story Points")
plt.show()
import logging;
logging.getLogger().setLevel(logging.CRITICAL)
plt.figure(figsize=(8, 6))
value_count.plot(kind='bar')
plt.title("Value Counts dos Story Points do Projeto 7764")
plt.xlabel("Valores do Story Point")
plt.ylabel("Contagem de vezes dos Story Points")
plt.show()
import logging;
logging.getLogger().setLevel(logging.FATAL)
plt.figure(figsize=(8, 6))
value_count.plot(kind='bar')
plt.title("Value Counts dos Story Points do Projeto 7764")
plt.xlabel("Valores do Story Point")
plt.ylabel("Contagem de vezes dos Story Points")
plt.show()
plt.figure(figsize=(8, 6))
value_count.plot(kind='bar')
plt.title("Value Counts dos Story Points do Projeto 7764")
plt.xlabel("Valores do Story Point")
plt.ylabel("Contagem de vezes dos Story Points")
plt.show()
plt.close()
plt.figure(figsize=(8, 6))
value_count.plot(kind='bar')
plt.title("Value Counts dos Story Points do Projeto 7764")
plt.xlabel("Valores do Story Point")
plt.ylabel("Contagem de vezes dos Story Points")
plt.show()
import logging;
logging.getLogger().setLevel(logging.FATAL)
plt.figure(figsize=(8, 6))
value_count.plot(kind='bar')
plt.title("Value Counts dos Story Points do Projeto 7764")
plt.xlabel("Valores do Story Point")
plt.ylabel("Contagem de vezes dos Story Points")
plt.show()
import logging;
logging.getLogger().setLevel(logging.INFO)
plt.figure(figsize=(8, 6))
value_count.plot(kind='bar')
plt.title("Value Counts dos Story Points do Projeto 7764")
plt.xlabel("Valores do Story Point")
plt.ylabel("Contagem de vezes dos Story Points")
plt.show()
plt.figure(figsize=(8, 6))
value_count.plot(kind='bar')
plt.title("Value Counts dos Story Points do Projeto 7764")
plt.xlabel("Valores do Story Point")
plt.ylabel("Contagem de vezes dos Story Points")
plt.show()
import logging;
logging.getLogger().setLevel(logging.CRITICAL)
plt.figure(figsize=(8, 6))
value_count.plot(kind='bar')
plt.title("Value Counts dos Story Points do Projeto 7764")
plt.xlabel("Valores do Story Point")
plt.ylabel("Contagem de vezes dos Story Points")
plt.show()
reticulate::repl_python()
import pandas as pd
import matplotlib.pyplot as plt
from textblob import TextBlob
import textstat
from sklearn.svm import SVR
from sklearn.feature_extraction.text import TfidfVectorizer
import logging;
logging.getLogger().setLevel(logging.CRITICAL)
name = '7764'
filename = '../data/{}.csv'.format(name)
df = pd.read_csv(filename)
df.head()
df.shape
df["context"] = df["title"] + df["description"]
df = df.drop(['created', 'issuekey', 'title', 'description'], axis=1)
df['context'] = df['context'].astype(str)
df.head()
value_count = df['storypoints'].value_counts().sort_index()
df_value_count = pd.DataFrame(data = {'valures': value_count.index, 'Contagem': value_count.values})
df_value_count
df['storypoints'].describe()
plt.figure(figsize=(8, 6))
value_count.plot(kind='bar')
plt.title('Value Counts dos Story Points do Projeto 7764')
plt.xlabel('Valores do Story Point')
plt.ylabel('Contagem de vezes dos Story Points')
plt.show()
plt.figure(figsize=(8, 2))
plt.boxplot(df['storypoints'], vert=False)
plt.title('Boxplot da Coluna Story Point')
plt.xlabel('Valores do Story Point')
plt.show()
mean = df['storypoints'].mean()
std_dev = df['storypoints'].std()
outlier_cutoff = 2 * std_dev
df_clean = df[(df['storypoints'] >= mean - outlier_cutoff) & (df['storypoints'] <= mean + outlier_cutoff)]
df_clean.head()
plt.figure(figsize=(8,2))
plt.boxplot(df_clean['storypoints'], vert=False)
plt.title('Boxplot dos Story Points do Projeto 7764 depois da remoção dos outliers')
plt.show()
value_count = df_clean['storypoints'].value_counts().sort_index()
plt.figure(figsize=(8, 6))
value_count.plot(kind='bar')
plt.title('Value Counts dos Story Points do Projeto 7764')
plt.xlabel('Valores do Story Point')
plt.ylabel('Contagem de vezes dos Story Points')
plt.show()
# separação treino e teste
num_linhas_treino = int(len(df_clean) * 0.7)
dados_treino = df_clean.iloc[:num_linhas_treino]
dados_teste = df_clean.iloc[num_linhas_treino:]
media_sp = dados_treino['storypoints'].mean()
media_sp
def mean_absolute_error(y_true, y_pred):
"""
Calcula o Mean Absolute Error (MAE), entre os valores verdadeiros (y_true) e os valores previstos (y_pred)
Args:
y_true: Uma lista ou array Numpy dos valores verdadeiros.
y_pred: Uma lista ou array NumPy dos valores previstos.
Returns:
mae: O Mean Absolute Error entre y_true e y_pred
"""
if len(y_true) != len(y_pred):
raise ValueError('Os tamanhos de y_true e y_pred devem ser iguais')
absolute_Errors =[abs(true-pred) for true, pred in zip(y_true, y_pred)]
mae = sum(absolute_Errors) / len(y_true)
return mae
lista_y_pred = [media_sp] * len(dados_teste)
mae_media_sp = mean_absolute_error(dados_teste['storypoints'], lista_y_pred)
mae_media_sp
df_results = pd.DataFrame(data=[['mae_media_sp', mae_media_sp, 'blue']], columns=['modelo', 'MAE Teste', "color"])
df_results
colunas = ['gunning_fog', 'polarity','subjectivity']
dados_treino['gunning_fog'] = dados_treino['context'].apply(textstat.gunning_fog)
dados_treino['polarity'] = dados_treino['context'].apply(lambda x: TextBlob(x).sentiment.polarity)
dados_treino['subjectivity'] = dados_treino['context'].apply(lambda x: TextBlob(x).sentiment.subjectivity)
dados_teste['gunning_fog'] = dados_teste['context'].apply(textstat.gunning_fog)
dados_teste['polarity'] = dados_teste['context'].apply(lambda x: TextBlob(x).sentiment.polarity)
dados_teste['subjectivity'] = dados_teste['context'].apply(lambda x: TextBlob(x).sentiment.subjectivity)
model = SVR()
model.fit(dados_treino[colunas], dados_treino['storypoints'])
y_pred = model.predict(dados_teste[colunas])
mae_leg = mean_absolute_error(dados_teste['storypoints'], y_pred)
df_results = df_results.append({'modelo':'mae_legibility', 'MAE Teste': mae_leg}, ignore_index=True)
df_results
vec = TfidfVectorizer(max_features=50)
tfidf_matrix_treino = vec.fit_transform(dados_treino['context'])
tfidf_matrix_teste = vec.transform(dados_teste['context'])
model = SVR()
model.fit(tfidf_matrix_treino, dados_treino['storypoints'])
y_pred = model.predict(tfidf_matrix_teste)
mae_tfidf = mean_absolute_error(dados_teste['storypoints'], y_pred)
df_results = df_results.append({'modelo':'mae_tfidf', 'MAE Teste': mae_tfidf}, ignore_index=True)
df_results
plt.figure()
df_results = df_results.sort_values(by='MAE Teste')
plt.bar(df_results['modelo'], df_results['MAE Teste'])
plt.title('Comparação do MAE entre os modelos')
plt.xlabel('Modelos')
plt.ylabel('MAE')
plt.show()
plt.figure()
df_results = df_results.sort_values(by='MAE Teste')
plt.bar(df_results['modelo'], df_results['MAE Teste'], colors=df_results['colors'])
plt.title('Comparação do MAE entre os modelos')
plt.xlabel('Modelos')
plt.ylabel('MAE')
plt.show()
plt.figure()
df_results = df_results.sort_values(by='MAE Teste')
plt.bar(df_results['modelo'], df_results['MAE Teste'], color=df_results['colors'])
plt.title('Comparação do MAE entre os modelos')
plt.xlabel('Modelos')
plt.ylabel('MAE')
plt.show()
plt.figure()
df_results = df_results.sort_values(by='MAE Teste')
plt.bar(df_results['modelo'], df_results['MAE Teste'], color=df_results['color'])
plt.title('Comparação do MAE entre os modelos')
plt.xlabel('Modelos')
plt.ylabel('MAE')
plt.show()
reticulate::repl_python()
import pandas as pd
import matplotlib.pyplot as plt
from textblob import TextBlob
import textstat
from sklearn.svm import SVR
from sklearn.feature_extraction.text import TfidfVectorizer
import logging;
logging.getLogger().setLevel(logging.CRITICAL)
name = '7764'
filename = '../data/{}.csv'.format(name)
df = pd.read_csv(filename)
df.head()
df.shape
df["context"] = df["title"] + df["description"]
df = df.drop(['created', 'issuekey', 'title', 'description'], axis=1)
df['context'] = df['context'].astype(str)
df.head()
value_count = df['storypoints'].value_counts().sort_index()
df_value_count = pd.DataFrame(data = {'valures': value_count.index, 'Contagem': value_count.values})
df_value_count
df['storypoints'].describe()
plt.figure(figsize=(8, 6))
value_count.plot(kind='bar')
plt.title('Value Counts dos Story Points do Projeto 7764')
plt.xlabel('Valores do Story Point')
plt.ylabel('Contagem de vezes dos Story Points')
plt.show()
plt.figure(figsize=(8, 2))
plt.boxplot(df['storypoints'], vert=False)
plt.title('Boxplot da Coluna Story Point')
plt.xlabel('Valores do Story Point')
plt.show()
mean = df['storypoints'].mean()
std_dev = df['storypoints'].std()
outlier_cutoff = 2 * std_dev
df_clean = df[(df['storypoints'] >= mean - outlier_cutoff) & (df['storypoints'] <= mean + outlier_cutoff)]
df_clean.head()
plt.figure(figsize=(8,2))
plt.boxplot(df_clean['storypoints'], vert=False)
plt.title('Boxplot dos Story Points do Projeto 7764 depois da remoção dos outliers')
plt.show()
value_count = df_clean['storypoints'].value_counts().sort_index()
plt.figure(figsize=(8, 6))
value_count.plot(kind='bar')
plt.title('Value Counts dos Story Points do Projeto 7764')
plt.xlabel('Valores do Story Point')
plt.ylabel('Contagem de vezes dos Story Points')
plt.show()
# separação treino e teste
num_linhas_treino = int(len(df_clean) * 0.7)
dados_treino = df_clean.iloc[:num_linhas_treino]
dados_teste = df_clean.iloc[num_linhas_treino:]
media_sp = dados_treino['storypoints'].mean()
media_sp
def mean_absolute_error(y_true, y_pred):
"""
Calcula o Mean Absolute Error (MAE), entre os valores verdadeiros (y_true) e os valores previstos (y_pred)
Args:
y_true: Uma lista ou array Numpy dos valores verdadeiros.
y_pred: Uma lista ou array NumPy dos valores previstos.
Returns:
mae: O Mean Absolute Error entre y_true e y_pred
"""
if len(y_true) != len(y_pred):
raise ValueError('Os tamanhos de y_true e y_pred devem ser iguais')
absolute_Errors =[abs(true-pred) for true, pred in zip(y_true, y_pred)]
mae = sum(absolute_Errors) / len(y_true)
return mae
lista_y_pred = [media_sp] * len(dados_teste)
mae_media_sp = mean_absolute_error(dados_teste['storypoints'], lista_y_pred)
mae_media_sp
df_results = pd.DataFrame(data=[['mae_media_sp', mae_media_sp, 'blue']], columns=['modelo', 'MAE Teste', "color"])
df_results
colunas = ['gunning_fog', 'polarity','subjectivity']
dados_treino['gunning_fog'] = dados_treino['context'].apply(textstat.gunning_fog)
dados_treino['polarity'] = dados_treino['context'].apply(lambda x: TextBlob(x).sentiment.polarity)
dados_treino['subjectivity'] = dados_treino['context'].apply(lambda x: TextBlob(x).sentiment.subjectivity)
dados_teste['gunning_fog'] = dados_teste['context'].apply(textstat.gunning_fog)
dados_teste['polarity'] = dados_teste['context'].apply(lambda x: TextBlob(x).sentiment.polarity)
dados_teste['subjectivity'] = dados_teste['context'].apply(lambda x: TextBlob(x).sentiment.subjectivity)
model = SVR()
model.fit(dados_treino[colunas], dados_treino['storypoints'])
y_pred = model.predict(dados_teste[colunas])
mae_leg = mean_absolute_error(dados_teste['storypoints'], y_pred)
df_results = df_results.append({'modelo':'mae_legibility', 'MAE Teste': mae_leg, 'color': 'orange'}, ignore_index=True)
df_results
vec = TfidfVectorizer(max_features=50)
tfidf_matrix_treino = vec.fit_transform(dados_treino['context'])
tfidf_matrix_teste = vec.transform(dados_teste['context'])
model = SVR()
model.fit(tfidf_matrix_treino, dados_treino['storypoints'])
y_pred = model.predict(tfidf_matrix_teste)
mae_tfidf = mean_absolute_error(dados_teste['storypoints'], y_pred)
df_results = df_results.append({'modelo':'mae_tfidf', 'MAE Teste': mae_tfidf, 'color': 'green'}, ignore_index=True)
df_results
plt.figure()
df_results = df_results.sort_values(by='MAE Teste')
plt.bar(df_results['modelo'], df_results['MAE Teste'], color=df_results['color'])
plt.title('Comparação do MAE entre os modelos')
plt.xlabel('Modelos')
plt.ylabel('MAE')
plt.show()
reticulate::repl_python()
import pandas as pd
import matplotlib.pyplot as plt
from textblob import TextBlob
import textstat
from sklearn.svm import SVR
from sklearn.feature_extraction.text import TfidfVectorizer
import logging;
logging.getLogger().setLevel(logging.CRITICAL)
name = '7764'
filename = '../data/{}.csv'.format(name)
df = pd.read_csv(filename)
df.head()
df.shape
df["context"] = df["title"] + df["description"]
df = df.drop(['created', 'issuekey', 'title', 'description'], axis=1)
df['context'] = df['context'].astype(str)
df.head()
value_count = df['storypoints'].value_counts().sort_index()
df_value_count = pd.DataFrame(data = {'valures': value_count.index, 'Contagem': value_count.values})
df_value_count
df['storypoints'].describe()
plt.figure(figsize=(8, 6))
value_count.plot(kind='bar')
plt.title('Value Counts dos Story Points do Projeto 7764')
plt.xlabel('Valores do Story Point')
plt.ylabel('Contagem de vezes dos Story Points')
plt.show()
plt.figure(figsize=(8, 2))
plt.boxplot(df['storypoints'], vert=False)
plt.title('Boxplot da Coluna Story Point')
plt.xlabel('Valores do Story Point')
plt.show()
mean = df['storypoints'].mean()
std_dev = df['storypoints'].std()
outlier_cutoff = 2 * std_dev
df_clean = df[(df['storypoints'] >= mean - outlier_cutoff) & (df['storypoints'] <= mean + outlier_cutoff)]
df_clean.head()
plt.figure(figsize=(8,2))
plt.boxplot(df_clean['storypoints'], vert=False)
plt.title('Boxplot dos Story Points do Projeto 7764 depois da remoção dos outliers')
plt.show()
value_count = df_clean['storypoints'].value_counts().sort_index()
plt.figure(figsize=(8, 6))
value_count.plot(kind='bar')
plt.title('Value Counts dos Story Points do Projeto 7764')
plt.xlabel('Valores do Story Point')
plt.ylabel('Contagem de vezes dos Story Points')
plt.show()
# separação treino e teste
num_linhas_treino = int(len(df_clean) * 0.7)
dados_treino = df_clean.iloc[:num_linhas_treino]
dados_teste = df_clean.iloc[num_linhas_treino:]
media_sp = dados_treino['storypoints'].mean()
media_sp
def mean_absolute_error(y_true, y_pred):
"""
Calcula o Mean Absolute Error (MAE), entre os valores verdadeiros (y_true) e os valores previstos (y_pred)
Args:
y_true: Uma lista ou array Numpy dos valores verdadeiros.
y_pred: Uma lista ou array NumPy dos valores previstos.
Returns:
mae: O Mean Absolute Error entre y_true e y_pred
"""
if len(y_true) != len(y_pred):
raise ValueError('Os tamanhos de y_true e y_pred devem ser iguais')
absolute_Errors =[abs(true-pred) for true, pred in zip(y_true, y_pred)]
mae = sum(absolute_Errors) / len(y_true)
return mae
lista_y_pred = [media_sp] * len(dados_teste)
mae_media_sp = mean_absolute_error(dados_teste['storypoints'], lista_y_pred)
mae_media_sp
df_results = pd.DataFrame(data=[['Media', mae_media_sp, 'blue']], columns=['modelo', 'MAE Teste', "color"])
df_results
colunas = ['gunning_fog', 'polarity','subjectivity']
dados_treino['gunning_fog'] = dados_treino['context'].apply(textstat.gunning_fog)
dados_treino['polarity'] = dados_treino['context'].apply(lambda x: TextBlob(x).sentiment.polarity)
dados_treino['subjectivity'] = dados_treino['context'].apply(lambda x: TextBlob(x).sentiment.subjectivity)
dados_teste['gunning_fog'] = dados_teste['context'].apply(textstat.gunning_fog)
dados_teste['polarity'] = dados_teste['context'].apply(lambda x: TextBlob(x).sentiment.polarity)
dados_teste['subjectivity'] = dados_teste['context'].apply(lambda x: TextBlob(x).sentiment.subjectivity)
model = SVR()
model.fit(dados_treino[colunas], dados_treino['storypoints'])
y_pred = model.predict(dados_teste[colunas])
mae_leg = mean_absolute_error(dados_teste['storypoints'], y_pred)
df_results = df_results.append({'modelo':'Legibility SVM', 'MAE Teste': mae_leg, 'color': 'orange'}, ignore_index=True)
df_results
vec = TfidfVectorizer(max_features=50)
tfidf_matrix_treino = vec.fit_transform(dados_treino['context'])
tfidf_matrix_teste = vec.transform(dados_teste['context'])
model = SVR()
model.fit(tfidf_matrix_treino, dados_treino['storypoints'])
y_pred = model.predict(tfidf_matrix_teste)
mae_tfidf = mean_absolute_error(dados_teste['storypoints'], y_pred)
df_results = df_results.append({'modelo':'TF-IDF SVM', 'MAE Teste': mae_tfidf, 'color': 'green'}, ignore_index=True)
df_results
plt.figure()
df_results = df_results.sort_values(by='MAE Teste')
plt.bar(df_results['modelo'], df_results['MAE Teste'], color=df_results['color'])
plt.title('Comparação do MAE entre os modelos')
plt.xlabel('Modelos')
plt.ylabel('MAE')
plt.show()
reticulate::repl_python()
import pandas as pd
import matplotlib.pyplot as plt
from textblob import TextBlob
import textstat
from sklearn.svm import SVR
from sklearn.feature_extraction.text import TfidfVectorizer
name = '7764'
filename = '../data/{}.csv'.format(name)
df = pd.read_csv(filename)
df.head()
df["context"] = df["title"] + df["description"]
df = df.drop(['created', 'issuekey', 'title', 'description'], axis=1)
df['context'] = df['context'].astype(str)
df.head()
mean = df['storypoints'].mean()
std_dev = df['storypoints'].std()
outlier_cutoff = 2 * std_dev
df_clean = df[(df['storypoints'] >= mean - outlier_cutoff) & (df['storypoints'] < mean + outlier_cutoff)]
df_clean.head()
name = '7764'
filename = '../data/{}.csv'.format(name)
df = pd.read_csv(filename)
quit
quit
knitr::kable(head(df)[,c("issuekey",	"title", "description", "storypoint")])
knitr::kable(head(df))
reticulate::repl_python()
reticulate::repl_python()
import pandas as pd
import matplotlib.pyplot as plt
from textblob import TextBlob
import textstat
from sklearn.svm import SVR
from sklearn.feature_extraction.text import TfidfVectorizer
name = '7764'
filename = '../data/{}.csv'.format(name)
df = pd.read_csv(filename)
df
quit
quit
knitr::kable(head(df)[,c("issuekey", "storypoints")])
knitr::kable(df)
knitr::kable(py$df)
reticulate::repl_python()
reticulate::repl_python()
df.head(2)
df["issuekey","title","storypoints"].head(2)
df["issuekey","title"].head(2)
df["title"].head(2)
df["title", "storypoints"].head(2)
df["title", "storypoints"].head(2)
df[["title", "storypoints"]].head(2)
df[["issuekey","title", "storypoints"]].head(2)
df[["issuekey","title", "description", "storypoints"]].head(2)
name = '7764'
filename = '../data/{}.csv'.format(name)
df = pd.read_csv(filename)
name = '7764'
filename = '../data/{}.csv'.format(name)
df = pd.read_csv(filename)
df[["issuekey","title", "description", "storypoints"]].head(2)
